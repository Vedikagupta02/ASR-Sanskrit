{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13462375,"sourceType":"datasetVersion","datasetId":8545375}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 0 ‚Äî run once\n!pip install jiwer\n!pip install -q librosa==0.9.2 jiwer==2.7.0 tqdm\n# (evaluate or datasets not required here; we use jiwer for WER/CER)\n\nimport os\nimport random\nimport json\nimport glob\nimport math\nimport time\nfrom pathlib import Path\nfrom typing import List, Dict\n\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torchaudio\nfrom torch import nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nfrom jiwer import wer, cer\n\n# reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:04:30.081851Z","iopub.execute_input":"2025-10-28T10:04:30.082411Z","iopub.status.idle":"2025-10-28T10:04:36.670222Z","shell.execute_reply.started":"2025-10-28T10:04:30.082387Z","shell.execute_reply":"2025-10-28T10:04:36.669439Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.3.0)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.14.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\nDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\nDownloading rapidfuzz-3.14.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-4.0.0 rapidfuzz-3.14.1\n\u001b[31mERROR: Could not find a version that satisfies the requirement jiwer==2.7.0 (from versions: 1.2, 1.3, 1.3.1, 1.3.2, 2.0.0, 2.0.1, 2.1.0, 2.2.0, 2.2.1, 2.3.0, 2.4.0, 2.5.0, 2.5.1, 2.5.2, 2.6.0, 3.0.0, 3.0.1, 3.0.2, 3.0.3, 3.0.4, 3.0.5, 3.1.0, 4.0.0)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for jiwer==2.7.0\u001b[0m\u001b[31m\n\u001b[0mDevice: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 1\nROOT = Path(\"/kaggle/input/sanskrit/Audio_Dataset\")\nprint(\"Dataset root:\", ROOT)\nprint(\"Exists:\", ROOT.exists())\nfor p in sorted(ROOT.iterdir()):\n    print(p.name, \"‚Üí\", len(list(p.iterdir())) if p.is_dir() else \"file\")\n    \n# print small listing from each folder\nfor folder in [\"sp002\", \"sp025\", \"Transcript\"]:\n    path = ROOT / folder\n    print(\"\\n\", folder, \"exists:\", path.exists())\n    if path.exists():\n        print(\" example:\", list(path.glob(\"*\"))[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:04:55.303978Z","iopub.execute_input":"2025-10-28T10:04:55.304763Z","iopub.status.idle":"2025-10-28T10:04:55.541274Z","shell.execute_reply.started":"2025-10-28T10:04:55.304721Z","shell.execute_reply":"2025-10-28T10:04:55.540492Z"}},"outputs":[{"name":"stdout","text":"Dataset root: /kaggle/input/sanskrit/Audio_Dataset\nExists: True\nTranscript ‚Üí 2\nsp002 ‚Üí 2705\nsp025 ‚Üí 923\n\n sp002 exists: True\n example: [PosixPath('/kaggle/input/sanskrit/Audio_Dataset/sp002/sp002-002227_RV_07.wav'), PosixPath('/kaggle/input/sanskrit/Audio_Dataset/sp002/sp002-002444_RV_07.wav'), PosixPath('/kaggle/input/sanskrit/Audio_Dataset/sp002/sp002-002145_RV_05.wav'), PosixPath('/kaggle/input/sanskrit/Audio_Dataset/sp002/sp002-001451_RV_05.wav'), PosixPath('/kaggle/input/sanskrit/Audio_Dataset/sp002/sp002-001347_RV_02.wav')]\n\n sp025 exists: True\n example: [PosixPath('/kaggle/input/sanskrit/Audio_Dataset/sp025/sp025-000731_004.wav'), PosixPath('/kaggle/input/sanskrit/Audio_Dataset/sp025/sp025-000704_004.wav'), PosixPath('/kaggle/input/sanskrit/Audio_Dataset/sp025/sp025-000305_002.wav'), PosixPath('/kaggle/input/sanskrit/Audio_Dataset/sp025/sp025-000304_002.wav'), PosixPath('/kaggle/input/sanskrit/Audio_Dataset/sp025/sp025-000609_004.wav')]\n\n Transcript exists: True\n example: [PosixPath('/kaggle/input/sanskrit/Audio_Dataset/Transcript/sp002.txt'), PosixPath('/kaggle/input/sanskrit/Audio_Dataset/Transcript/sp025.txt')]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 2 - create mapping if not present\nMAP_CSV = Path(\"audio_transcript_mapping.csv\")\nif MAP_CSV.exists():\n    df_mapping = pd.read_csv(MAP_CSV)\n    print(\"Loaded mapping:\", MAP_CSV, \"->\", len(df_mapping))\nelse:\n    # read transcripts (one file per speaker with lines)\n    trans_dir = ROOT / \"Transcript\"\n    def read_lines(p): \n        return [l.strip() for l in open(p, 'r', encoding='utf-8').read().splitlines() if l.strip()]\n    sp2 = read_lines(trans_dir / \"sp002.txt\")\n    sp25 = read_lines(trans_dir / \"sp025.txt\")\n    # gather audio files (sorted)\n    sp2_audio = sorted(map(str, (ROOT/\"sp002\").glob(\"*.wav\")))\n    sp25_audio = sorted(map(str, (ROOT/\"sp025\").glob(\"*.wav\")))\n    print(\"sp002 audio:\", len(sp2_audio), \"lines:\", len(sp2))\n    print(\"sp025 audio:\", len(sp25_audio), \"lines:\", len(sp25))\n    # pair by index (warn if counts mismatch)\n    pairs=[]\n    for i, a in enumerate(sp2_audio):\n        if i < len(sp2): pairs.append({\"audio_path\": a, \"transcript\": sp2[i], \"speaker\":\"sp002\"})\n    for i, a in enumerate(sp25_audio):\n        if i < len(sp25): pairs.append({\"audio_path\": a, \"transcript\": sp25[i], \"speaker\":\"sp025\"})\n    df_mapping = pd.DataFrame(pairs)\n    df_mapping.to_csv(MAP_CSV, index=False)\n    print(\"Saved mapping to\", MAP_CSV, \"Total pairs:\", len(df_mapping))\ndf_mapping.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:05:05.161641Z","iopub.execute_input":"2025-10-28T10:05:05.162262Z","iopub.status.idle":"2025-10-28T10:05:05.241621Z","shell.execute_reply.started":"2025-10-28T10:05:05.162231Z","shell.execute_reply":"2025-10-28T10:05:05.241017Z"}},"outputs":[{"name":"stdout","text":"sp002 audio: 2705 lines: 2705\nsp025 audio: 923 lines: 923\nSaved mapping to audio_transcript_mapping.csv Total pairs: 3628\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                          audio_path  \\\n0  /kaggle/input/sanskrit/Audio_Dataset/sp002/sp0...   \n1  /kaggle/input/sanskrit/Audio_Dataset/sp002/sp0...   \n2  /kaggle/input/sanskrit/Audio_Dataset/sp002/sp0...   \n3  /kaggle/input/sanskrit/Audio_Dataset/sp002/sp0...   \n4  /kaggle/input/sanskrit/Audio_Dataset/sp002/sp0...   \n\n                                          transcript speaker  \n0     sp002-000001_KS_04\\t‡§Æ‡•Ç‡§∞‡•ç‡§ö‡•ç‡§õ‡§ø‡§§‡§æ ‡§∞‡§§‡§ø‡§É ‡§á‡§§‡•ç‡§Ø‡•Å‡§ï‡•ç‡§§‡§Æ‡•ç   sp002  \n1     sp002-000002_KS_04\\t‡§∏‡§Æ‡•ç‡§™‡•ç‡§∞‡§§‡§ø ‡§§‡§¶‡•ç‡§µ‡•É‡§§‡•ç‡§§‡§æ‡§®‡•ç‡§§‡§Æ‡•á‡§µ‡§æ‡§π   sp002  \n2                          sp002-000003_KS_04\\t‡§Ö‡§•‡•á‡§§‡§ø   sp002  \n3  sp002-000004_KS_04\\t‡§Ö‡§• ‡§Ö‡§®‡§®‡•ç‡§§‡§∞‡§Æ‡•ç ‡§Æ‡•ã‡§π‡•ã ‡§Æ‡•Ç‡§∞‡•ç‡§ö‡•ç‡§õ‡§æ ...   sp002  \n4  sp002-000005_KS_04\\t‡§™‡§∞‡§æ‡§Ø‡§£‡§Æ‡•ç ‡§Ö‡§≠‡§ø‡§™‡•ç‡§∞‡•á‡§§‡•á ‡§§‡§§‡•ç‡§™‡§∞‡•á ‡§™...   sp002  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>audio_path</th>\n      <th>transcript</th>\n      <th>speaker</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/input/sanskrit/Audio_Dataset/sp002/sp0...</td>\n      <td>sp002-000001_KS_04\\t‡§Æ‡•Ç‡§∞‡•ç‡§ö‡•ç‡§õ‡§ø‡§§‡§æ ‡§∞‡§§‡§ø‡§É ‡§á‡§§‡•ç‡§Ø‡•Å‡§ï‡•ç‡§§‡§Æ‡•ç</td>\n      <td>sp002</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/input/sanskrit/Audio_Dataset/sp002/sp0...</td>\n      <td>sp002-000002_KS_04\\t‡§∏‡§Æ‡•ç‡§™‡•ç‡§∞‡§§‡§ø ‡§§‡§¶‡•ç‡§µ‡•É‡§§‡•ç‡§§‡§æ‡§®‡•ç‡§§‡§Æ‡•á‡§µ‡§æ‡§π</td>\n      <td>sp002</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/input/sanskrit/Audio_Dataset/sp002/sp0...</td>\n      <td>sp002-000003_KS_04\\t‡§Ö‡§•‡•á‡§§‡§ø</td>\n      <td>sp002</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/input/sanskrit/Audio_Dataset/sp002/sp0...</td>\n      <td>sp002-000004_KS_04\\t‡§Ö‡§• ‡§Ö‡§®‡§®‡•ç‡§§‡§∞‡§Æ‡•ç ‡§Æ‡•ã‡§π‡•ã ‡§Æ‡•Ç‡§∞‡•ç‡§ö‡•ç‡§õ‡§æ ...</td>\n      <td>sp002</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/input/sanskrit/Audio_Dataset/sp002/sp0...</td>\n      <td>sp002-000005_KS_04\\t‡§™‡§∞‡§æ‡§Ø‡§£‡§Æ‡•ç ‡§Ö‡§≠‡§ø‡§™‡•ç‡§∞‡•á‡§§‡•á ‡§§‡§§‡•ç‡§™‡§∞‡•á ‡§™...</td>\n      <td>sp002</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Cell 3\nfrom sklearn.model_selection import train_test_split\n\ndf = df_mapping.copy().dropna(subset=[\"audio_path\",\"transcript\"])\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df[\"speaker\"])\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED, stratify=temp_df[\"speaker\"])\nprint(len(train_df), len(val_df), len(test_df))\ntrain_df.to_csv(\"train_split.csv\", index=False)\nval_df.to_csv(\"val_split.csv\", index=False)\ntest_df.to_csv(\"test_split.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:05:17.822715Z","iopub.execute_input":"2025-10-28T10:05:17.823467Z","iopub.status.idle":"2025-10-28T10:05:18.363036Z","shell.execute_reply.started":"2025-10-28T10:05:17.823441Z","shell.execute_reply":"2025-10-28T10:05:18.362362Z"}},"outputs":[{"name":"stdout","text":"2902 363 363\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 4\nVOCAB_FILE = Path(\"vocab.json\")\nif VOCAB_FILE.exists():\n    with open(VOCAB_FILE, 'r', encoding='utf-8') as f:\n        vocab = json.load(f)\n    print(\"Loaded vocab.json size\", len(vocab))\nelse:\n    # build from transcripts (char-level)\n    all_text = \" \".join(df[\"transcript\"].astype(str).tolist())\n    unique_chars = sorted(set(all_text))\n    vocab = {ch: idx for idx, ch in enumerate(unique_chars)}\n    # add specials\n    vocab.update({\"[PAD]\": len(vocab), \"[UNK]\": len(vocab)+1, \"[BLANK]\": len(vocab)+2})\n    with open(VOCAB_FILE, 'w', encoding='utf-8') as f:\n        json.dump(vocab, f, ensure_ascii=False, indent=2)\n    print(\"Saved vocab.json size\", len(vocab))\n# invert for id->char\nid2char = {v:k for k,v in vocab.items()}\nvocab_size = len(vocab)\nprint(\"Vocab size\", vocab_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:05:25.995359Z","iopub.execute_input":"2025-10-28T10:05:25.995945Z","iopub.status.idle":"2025-10-28T10:05:26.014466Z","shell.execute_reply.started":"2025-10-28T10:05:25.995924Z","shell.execute_reply":"2025-10-28T10:05:26.013774Z"}},"outputs":[{"name":"stdout","text":"Saved vocab.json size 84\nVocab size 84\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# === REPLACEMENT CELL for Cell 5: robust torchaudio-based feature extraction + SpecAugment ===\nimport torchaudio\nimport torchaudio.functional as Fta\nfrom torchaudio.transforms import MelSpectrogram, AmplitudeToDB, Resample\nimport numpy as np\nimport torch\n\nSAMPLE_RATE = 16000\nN_MELS = 80\nN_FFT = 400         # 25 ms @ 16k\nHOP_LENGTH = 160    # 10 ms @ 16k\n\n# Precreate transforms (faster)\nmel_spec_transform = MelSpectrogram(\n    sample_rate=SAMPLE_RATE,\n    n_fft=N_FFT,\n    hop_length=HOP_LENGTH,\n    n_mels=N_MELS,\n    power=2.0,        # power spectrogram\n    normalized=False,\n)\ndb_transform = AmplitudeToDB(stype='power')\n\ndef wav_to_mel(path, sr=SAMPLE_RATE, n_mels=N_MELS):\n    \"\"\"\n    Load audio with torchaudio, compute Mel (dB) + delta + delta-delta,\n    normalize per-utterance and return stacked feature (240, T).\n    \"\"\"\n    # load waveform (channels, samples), torchaudio returns float32\n    waveform, sr0 = torchaudio.load(path)\n    # convert to mono\n    if waveform.size(0) > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)\n    # resample if needed\n    if sr0 != sr:\n        waveform = Resample(sr0, sr)(waveform)\n\n    # waveform shape: (1, samples)\n    # compute mel spectrogram -> (n_mels, time) after squeeze\n    mel = mel_spec_transform(waveform)          # (1, n_mels, T)\n    mel_db = db_transform(mel)                  # (1, n_mels, T)\n    mel_db = mel_db.squeeze(0)                  # (n_mels, T) as Tensor\n\n    # compute deltas using torchaudio.functional.compute_deltas\n    delta = Fta.compute_deltas(mel_db)          # (n_mels, T)\n    delta2 = Fta.compute_deltas(delta)          # (n_mels, T)\n\n    # convert to numpy and stack: (n_mels*3, T)\n    mel_np = mel_db.cpu().numpy()\n    delta_np = delta.cpu().numpy()\n    delta2_np = delta2.cpu().numpy()\n    stacked = np.concatenate([mel_np, delta_np, delta2_np], axis=0)  # (240, T)\n\n    # per-utterance normalization (safe)\n    stacked = (stacked - stacked.mean()) / (stacked.std() + 1e-9)\n    return stacked.astype(np.float32)  # (240, T)\n\n# SpecAugment (same semantics as before)\ndef spec_augment(spec, time_mask_param=30, freq_mask_param=13, num_time_masks=1, num_freq_masks=1):\n    \"\"\"\n    spec: numpy array (240, T)\n    Returns augmented numpy array.\n    \"\"\"\n    spec = spec.copy()\n    _, T = spec.shape\n    # time masks\n    for _ in range(num_time_masks):\n        t = np.random.randint(0, time_mask_param + 1)\n        if t <= 0 or T - t <= 0:\n            continue\n        t0 = np.random.randint(0, T - t)\n        spec[:, t0:t0+t] = 0\n    # freq masks applied only to mel bands rows 0:80\n    for _ in range(num_freq_masks):\n        f = np.random.randint(0, freq_mask_param + 1)\n        if f <= 0 or 80 - f <= 0:\n            continue\n        f0 = np.random.randint(0, 80 - f)\n        spec[f0:f0+f, :] = 0\n    return spec\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:08:28.512901Z","iopub.execute_input":"2025-10-28T10:08:28.513703Z","iopub.status.idle":"2025-10-28T10:08:28.561175Z","shell.execute_reply.started":"2025-10-28T10:08:28.513678Z","shell.execute_reply":"2025-10-28T10:08:28.560533Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Cell 6\ndef text_to_ids(text, vocab_map):\n    ids = []\n    for ch in text:\n        ids.append(vocab_map.get(ch, vocab_map.get(\"[UNK]\")))\n    return ids\n\nclass SanskritSpectrogramDataset(Dataset):\n    def __init__(self, df, vocab_map, augment=False):\n        self.df = df.reset_index(drop=True)\n        self.vocab = vocab_map\n        self.augment = augment\n    def __len__(self): return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        spec = wav_to_mel(row['audio_path'])   # (240, T)\n        if self.augment:\n            spec = spec_augment(spec)\n        # ATS: transpose to (T, feat)\n        spec_t = spec.T   # (T, feat=240)\n        label_ids = text_to_ids(row['transcript'], self.vocab)\n        sample = {\"inputs\": spec_t, \"labels\": np.array(label_ids, dtype=np.int64), \"audio_path\": row['audio_path']}\n        return sample\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:08:31.226294Z","iopub.execute_input":"2025-10-28T10:08:31.226844Z","iopub.status.idle":"2025-10-28T10:08:31.233211Z","shell.execute_reply.started":"2025-10-28T10:08:31.226821Z","shell.execute_reply":"2025-10-28T10:08:31.232477Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Cell 7\ndef collate_fn(batch):\n    # batch: list of {\"inputs\": (T, feat), \"labels\": (L,), ...}\n    batch_inputs = [torch.tensor(b[\"inputs\"], dtype=torch.float32) for b in batch]\n    lengths = torch.tensor([t.shape[0] for t in batch_inputs], dtype=torch.long)\n    # pad inputs on time dimension\n    max_len = max([t.shape[0] for t in batch_inputs])\n    feat_dim = batch_inputs[0].shape[1]\n    padded = torch.zeros((len(batch_inputs), max_len, feat_dim), dtype=torch.float32)\n    for i, t in enumerate(batch_inputs):\n        padded[i, :t.shape[0], :] = t\n    # labels padded with pad id\n    label_list = [torch.tensor(b[\"labels\"], dtype=torch.long) for b in batch]\n    max_lab = max([l.numel() for l in label_list])\n    pad_id = vocab[\"[PAD]\"]\n    label_padded = torch.full((len(label_list), max_lab), fill_value=pad_id, dtype=torch.long)\n    label_lens = torch.tensor([l.numel() for l in label_list], dtype=torch.long)\n    for i, l in enumerate(label_list):\n        label_padded[i, :l.numel()] = l\n    return {\n        \"inputs\": padded,             # (B, T, feat)\n        \"input_lengths\": lengths,     # (B,)\n        \"labels\": label_padded,       # (B, L)\n        \"label_lengths\": label_lens\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:08:33.796714Z","iopub.execute_input":"2025-10-28T10:08:33.797192Z","iopub.status.idle":"2025-10-28T10:08:33.803719Z","shell.execute_reply.started":"2025-10-28T10:08:33.797168Z","shell.execute_reply":"2025-10-28T10:08:33.803050Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Cell 8\ntrain_dataset = SanskritSpectrogramDataset(train_df, vocab, augment=True)\nval_dataset   = SanskritSpectrogramDataset(val_df, vocab, augment=False)\ntest_dataset  = SanskritSpectrogramDataset(test_df, vocab, augment=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=0)\nval_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\ntest_loader  = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n\nprint(\"train/val/test batches:\", len(train_loader), len(val_loader), len(test_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:09:33.313685Z","iopub.execute_input":"2025-10-28T10:09:33.314386Z","iopub.status.idle":"2025-10-28T10:09:33.320649Z","shell.execute_reply.started":"2025-10-28T10:09:33.314362Z","shell.execute_reply":"2025-10-28T10:09:33.320037Z"}},"outputs":[{"name":"stdout","text":"train/val/test batches: 726 91 91\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# ============================================\n# üß† HYBRID ASR MODEL ‚Äî BiLSTM + Attention + CTC\n# FP16 safe, AMP compatible, ready for Kaggle\n# ============================================\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport random\n\n# ---------------- Encoder ---------------- #\nclass EncoderBiLSTM(nn.Module):\n    def __init__(self, input_dim=240, cnn_channels=128, lstm_hidden=512, num_layers=2, dropout=0.3):\n        super().__init__()\n        # Convolutional frontend for local feature extraction\n        self.conv = nn.Sequential(\n            nn.Conv1d(input_dim, cnn_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.BatchNorm1d(cnn_channels)\n        )\n        # Bidirectional LSTM encoder\n        self.lstm = nn.LSTM(\n            input_size=cnn_channels,\n            hidden_size=lstm_hidden,\n            num_layers=num_layers,\n            bidirectional=True,\n            batch_first=True,\n            dropout=dropout\n        )\n        self.proj = nn.Linear(2 * lstm_hidden, 2 * lstm_hidden)\n\n    def forward(self, x, lengths):\n        # x: (B, T, feat)\n        x = x.transpose(1, 2)            # (B, feat, T)\n        x = self.conv(x)                 # (B, C, T)\n        x = x.transpose(1, 2)            # (B, T, C)\n\n        # pack sequences for LSTM\n        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        out_packed, _ = self.lstm(packed)\n        out, out_lengths = pad_packed_sequence(out_packed, batch_first=True)\n\n        out = self.proj(out)  # (B, T', 2*hidden)\n        return out, out_lengths\n\n\n# ---------------- Attention Decoder ---------------- #\nclass AttentionDecoder(nn.Module):\n    def __init__(self, enc_dim, hidden_dim, vocab_size, emb_dim=128):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=vocab[\"[PAD]\"])\n        self.attn = nn.Linear(enc_dim + emb_dim, hidden_dim)\n        self.v_attn = nn.Linear(hidden_dim, 1)\n        self.lstm = nn.LSTMCell(emb_dim + enc_dim, hidden_dim)\n        self.out = nn.Linear(hidden_dim, vocab_size)\n\n    def forward_step(self, prev_token, prev_hidden, encoder_outputs, mask):\n        # prev_token: (B,)\n        emb = self.embedding(prev_token)  # (B, emb)\n        B, T, C = encoder_outputs.size()\n\n        # Attention mechanism\n        emb_exp = emb.unsqueeze(1).expand(-1, T, -1)      # (B, T, emb)\n        att_in = torch.cat([encoder_outputs, emb_exp], 2) # (B, T, enc+emb)\n        energy = torch.tanh(self.attn(att_in))            # (B, T, hidden)\n        scores = self.v_attn(energy).squeeze(-1)          # (B, T)\n\n        # ===== FP16-safe masking =====\n        if mask.dtype != torch.bool:\n            mask = mask.bool()\n        neg_inf = torch.finfo(scores.dtype).min  # safe min value for dtype\n        scores = scores.masked_fill(~mask, neg_inf)\n        # ==============================\n\n        alpha = torch.softmax(scores, dim=1)              # (B, T)\n        context = torch.bmm(alpha.unsqueeze(1), encoder_outputs).squeeze(1)  # (B, enc_dim)\n\n        hx, cx = self.lstm(torch.cat([emb, context], dim=1), prev_hidden)\n        logits = self.out(hx)  # (B, vocab)\n        return logits, (hx, cx), alpha\n\n    def forward(self, targets, encoder_outputs, enc_mask, teacher_forcing_ratio=1.0):\n        B, L = targets.size()\n        device = targets.device\n        outputs = []\n        hx = torch.zeros(B, self.lstm.hidden_size, device=device)\n        cx = torch.zeros(B, self.lstm.hidden_size, device=device)\n        prev = torch.full((B,), vocab[\"[PAD]\"], dtype=torch.long, device=device)\n\n        for t in range(L):\n            logits, (hx, cx), alpha = self.forward_step(prev, (hx, cx), encoder_outputs, enc_mask)\n            outputs.append(logits.unsqueeze(1))\n            # Teacher forcing\n            use_teacher = random.random() < teacher_forcing_ratio\n            prev = targets[:, t] if use_teacher else logits.argmax(dim=-1)\n\n        outputs = torch.cat(outputs, dim=1)  # (B, L, vocab)\n        return outputs\n\n\n# ---------------- Hybrid Model ---------------- #\nclass HybridASRModel(nn.Module):\n    def __init__(self, vocab_size, enc_input_dim=240):\n        super().__init__()\n        self.encoder = EncoderBiLSTM(input_dim=enc_input_dim)\n        enc_dim = 2 * 512\n        self.ctc_fc = nn.Linear(enc_dim, vocab_size)\n        self.decoder = AttentionDecoder(enc_dim, hidden_dim=512, vocab_size=vocab_size)\n\n    def forward(self, inputs, input_lengths, labels=None, label_lengths=None, teacher_forcing_ratio=1.0):\n        enc_out, enc_lens = self.encoder(inputs, input_lengths)\n        ctc_logits = self.ctc_fc(enc_out)  # (B, T, V)\n\n        # Create attention mask safely\n        max_time = enc_out.size(1)\n        device = inputs.device\n        if isinstance(enc_lens, (tuple, list)):\n            enc_lens = torch.tensor(enc_lens, device=device)\n        else:\n            enc_lens = enc_lens.to(device)\n        enc_mask = torch.arange(max_time, device=device).unsqueeze(0) < enc_lens.unsqueeze(1)\n\n        attn_logits = None\n        if labels is not None:\n            attn_logits = self.decoder(labels, enc_out, enc_mask, teacher_forcing_ratio)\n        return ctc_logits, attn_logits, enc_lens\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:14:33.443984Z","iopub.execute_input":"2025-10-28T10:14:33.444308Z","iopub.status.idle":"2025-10-28T10:14:33.460873Z","shell.execute_reply.started":"2025-10-28T10:14:33.444287Z","shell.execute_reply":"2025-10-28T10:14:33.460065Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Cell 10\nctc_loss_fn = nn.CTCLoss(blank=vocab[\"[BLANK]\"], zero_infinity=True, reduction='mean')\n\ndef compute_losses(ctc_logits, attn_logits, enc_lens, labels, label_lens, alpha=0.4):\n    # ctc_logits: (B, T, V) -> for CTCLoss need (T, B, V)\n    log_probs = torch.log_softmax(ctc_logits, dim=-1)  # (B, T, V)\n    log_probs_TBV = log_probs.permute(1,0,2)           # (T, B, V)\n    input_lengths = enc_lens\n    target_lengths = label_lens\n    # flatten labels by removing pad (assumes pad id at end)\n    # CTC expects targets as 1D concatenated\n    targets = []\n    for i in range(labels.size(0)):\n        targets.append(labels[i, :label_lens[i]].cpu().numpy().tolist())\n    targets_concat = torch.tensor([t for seq in targets for t in seq], dtype=torch.long).to(log_probs.device)\n    # build targets offset: pass as CPU 1D? PyTorch CTC allows 1D targets\n    # But we will call with list approach: requires concat and lengths\n    # Use ctc_loss_fn\n    # PyTorch's CTCLoss requires targets as 1D tensor\n    ctc_loss = ctc_loss_fn(log_probs_TBV, targets_concat, input_lengths, target_lengths)\n    # Attention CE loss\n    attn_loss = torch.tensor(0.0, device=log_probs.device)\n    if attn_logits is not None:\n        # attn_logits: (B, L, V), labels: (B, L)\n        B, L, V = attn_logits.size()\n        attn_logits_flat = attn_logits.view(B*L, V)\n        labels_flat = labels.view(B*L)\n        # CE ignoring pad id\n        attn_loss = nn.functional.cross_entropy(attn_logits_flat, labels_flat, ignore_index=vocab[\"[PAD]\"])\n    total = alpha * ctc_loss + (1.0 - alpha) * attn_loss\n    return total, ctc_loss.detach(), attn_loss.detach()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:14:36.032286Z","iopub.execute_input":"2025-10-28T10:14:36.032893Z","iopub.status.idle":"2025-10-28T10:14:36.039406Z","shell.execute_reply.started":"2025-10-28T10:14:36.032868Z","shell.execute_reply":"2025-10-28T10:14:36.038625Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# === DEBUG: quick single-sample test (run after replacing wav_to_mel) ===\nfrom pathlib import Path\n# pick one audio path from df_mapping (ensure df_mapping exists)\nsample_path = df_mapping.iloc[0]['audio_path'] if 'df_mapping' in globals() else None\nprint(\"Sample audio:\", sample_path)\nif sample_path is not None:\n    feat = wav_to_mel(sample_path)\n    print(\"Feature shape (240, T):\", feat.shape)\n    # try augment\n    aug = spec_augment(feat)\n    print(\"Augmented shape:\", aug.shape)\nelse:\n    print(\"df_mapping not found in memory ‚Äî re-run mapping cell first.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:14:38.286153Z","iopub.execute_input":"2025-10-28T10:14:38.286448Z","iopub.status.idle":"2025-10-28T10:14:38.317448Z","shell.execute_reply.started":"2025-10-28T10:14:38.286429Z","shell.execute_reply":"2025-10-28T10:14:38.316837Z"}},"outputs":[{"name":"stdout","text":"Sample audio: /kaggle/input/sanskrit/Audio_Dataset/sp002/sp002-000001_KS_04.wav\nFeature shape (240, T): (240, 306)\nAugmented shape: (240, 306)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Cell 11\nmodel = HybridASRModel(vocab_size).to(DEVICE)\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n\nNUM_EPOCHS = 10\nALPHA = 0.4   # weight for CTC\nbest_wer = 1.0\nsave_dir = Path(\"./hybrid_checkpoints\")\nsave_dir.mkdir(exist_ok=True)\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", leave=False)\n    running_loss = 0.0\n    for batch in pbar:\n        inputs = batch[\"inputs\"].to(DEVICE)        # (B, T, feat)\n        in_lens = batch[\"input_lengths\"].to(DEVICE)\n        labels = batch[\"labels\"].to(DEVICE)\n        lab_lens = batch[\"label_lengths\"].to(DEVICE)\n\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n            ctc_logits, attn_logits, enc_lens = model(inputs, in_lens, labels, lab_lens, teacher_forcing_ratio=0.9)\n            loss, ctc_l, attn_l = compute_losses(ctc_logits, attn_logits, enc_lens, labels, lab_lens, alpha=ALPHA)\n\n        scaler.scale(loss).backward()\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item()\n        pbar.set_postfix({\"loss\": f\"{running_loss / (pbar.n + 1):.4f}\"})\n\n    # --- validation at epoch end (you can also evaluate every N steps)\n    model.eval()\n    all_preds = []\n    all_refs = []\n    with torch.no_grad():\n        for vb in tqdm(val_loader, desc=\"Val\", leave=False):\n            v_inputs = vb[\"inputs\"].to(DEVICE)\n            v_in_lens = vb[\"input_lengths\"].to(DEVICE)\n            v_labels = vb[\"labels\"].to(DEVICE)\n            v_label_lens = vb[\"label_lengths\"].to(DEVICE)\n            # forward without teacher forcing (greedy decode)\n            ctc_logits, attn_logits, enc_lens = model(v_inputs, v_in_lens, labels=None, label_lengths=None, teacher_forcing_ratio=0.0)\n            # decode CTC greedy on ctc_logits\n            probs = torch.softmax(ctc_logits, dim=-1)  # (B,T,V)\n            preds = probs.argmax(dim=-1).cpu().numpy()  # (B,T)\n            # collapse repeats & remove blank\n            for i, p in enumerate(preds):\n                prev = None\n                out = []\n                for tok in p:\n                    if tok == vocab[\"[BLANK]\"]: \n                        prev = tok; continue\n                    if tok == prev: \n                        prev = tok; continue\n                    out.append(id2char.get(int(tok), \"\"))\n                    prev = tok\n                pred_str = \"\".join(out)\n                # ground truth\n                lab = v_labels[i, :v_label_lens[i]].cpu().numpy()\n                ref_str = \"\".join([id2char.get(int(x), \"\") for x in lab])\n                all_preds.append(pred_str)\n                all_refs.append(ref_str)\n    val_wer = wer(all_refs, all_preds)\n    val_cer = cer(all_refs, all_preds)\n    print(f\"Epoch {epoch+1} ‚Äî AvgLoss: {running_loss/len(train_loader):.4f}  Val WER: {val_wer:.4f}  CER: {val_cer:.4f}\")\n    # checkpoint if better\n    if val_wer < best_wer:\n        best_wer = val_wer\n        torch.save(model.state_dict(), save_dir / \"best_model.pt\")\n        print(\"Saved best model (WER improved).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:14:40.671985Z","iopub.execute_input":"2025-10-28T10:14:40.672462Z","iopub.status.idle":"2025-10-28T11:18:28.240401Z","shell.execute_reply.started":"2025-10-28T10:14:40.672441Z","shell.execute_reply":"2025-10-28T11:18:28.239540Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_37/1507901773.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/10:   0%|          | 0/726 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_37/1507901773.py:23: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1 ‚Äî AvgLoss: 2.9604  Val WER: 1.0000  CER: 0.9092\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/10:   0%|          | 0/726 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2 ‚Äî AvgLoss: 2.0597  Val WER: 0.9991  CER: 0.5993\nSaved best model (WER improved).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/10:   0%|          | 0/726 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3 ‚Äî AvgLoss: 1.7329  Val WER: 0.9650  CER: 0.3522\nSaved best model (WER improved).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/10:   0%|          | 0/726 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4 ‚Äî AvgLoss: 1.5555  Val WER: 0.9156  CER: 0.2579\nSaved best model (WER improved).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/10:   0%|          | 0/726 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5 ‚Äî AvgLoss: 1.4666  Val WER: 0.8721  CER: 0.2267\nSaved best model (WER improved).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/10:   0%|          | 0/726 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6 ‚Äî AvgLoss: 1.3932  Val WER: 0.8245  CER: 0.1998\nSaved best model (WER improved).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/10:   0%|          | 0/726 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7 ‚Äî AvgLoss: 1.3263  Val WER: 0.8079  CER: 0.1979\nSaved best model (WER improved).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/10:   0%|          | 0/726 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8 ‚Äî AvgLoss: 1.2612  Val WER: 0.7846  CER: 0.1884\nSaved best model (WER improved).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/10:   0%|          | 0/726 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9 ‚Äî AvgLoss: 1.1721  Val WER: 0.7585  CER: 0.1756\nSaved best model (WER improved).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/10:   0%|          | 0/726 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Val:   0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10 ‚Äî AvgLoss: 1.0692  Val WER: 0.7262  CER: 0.1541\nSaved best model (WER improved).\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"!pip install -q kenlm pyctcdecode\n\nimport os, re, glob\n\n# Gather all transcript text\nTRANSCRIPT_PATH = \"/kaggle/input/sanskrit/Audio_Dataset/Transcript\"\ntexts = []\n\nfor txt_file in glob.glob(os.path.join(TRANSCRIPT_PATH, \"*.txt\")):\n    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n        lines = f.readlines()\n        for line in lines:\n            line = re.sub(r\"\\s+\", \" \", line.strip())\n            if len(line) > 0:\n                texts.append(line)\n\nprint(f\"Collected {len(texts)} lines for LM training\")\n\n# Save corpus\nwith open(\"lm_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\\n\".join(texts))\n\n# Train kenlm 3-gram model\n!lmplz -o 3 < lm_corpus.txt > sanskrit.arpa\n!build_binary sanskrit.arpa sanskrit.klm\n\nprint(\"‚úÖ Trained 3-gram Sanskrit LM: sanskrit.klm\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:19:09.066432Z","iopub.execute_input":"2025-10-28T11:19:09.066700Z","iopub.status.idle":"2025-10-28T11:20:09.563251Z","shell.execute_reply.started":"2025-10-28T11:19:09.066683Z","shell.execute_reply":"2025-10-28T11:20:09.562169Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m427.5/427.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m533.5/533.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollected 3628 lines for LM training\n/bin/bash: line 1: lmplz: command not found\n/bin/bash: line 1: build_binary: command not found\n‚úÖ Trained 3-gram Sanskrit LM: sanskrit.klm\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"from pyctcdecode import build_ctcdecoder\n\n# Load vocabulary\nlabels = list(vocab.keys())\n\n# Build beam search decoder WITHOUT LM\ndecoder = build_ctcdecoder(\n    labels=labels,\n    kenlm_model_path=None,  # no LM file\n    alpha=0.0,  # LM weight (unused)\n    beta=0.0   # word insertion penalty\n)\n\nprint(\"‚úÖ Beam search decoder (no LM) ready!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:21:32.865221Z","iopub.execute_input":"2025-10-28T11:21:32.866014Z","iopub.status.idle":"2025-10-28T11:21:32.871159Z","shell.execute_reply.started":"2025-10-28T11:21:32.865988Z","shell.execute_reply":"2025-10-28T11:21:32.870334Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Beam search decoder (no LM) ready!\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"from jiwer import wer, cer\nfrom tqdm import tqdm\nimport numpy as np\n\ndef evaluate_dataset(dataset, name=\"Validation\"):\n    model.eval()\n    all_preds, all_refs = [], []\n\n    with torch.no_grad():\n        for batch in tqdm(DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn, num_workers=0),\n                          desc=f\"Evaluating {name}\"):\n            inputs = batch[\"inputs\"].to(DEVICE)\n            in_lens = batch[\"input_lengths\"].to(DEVICE)\n            labels = batch[\"labels\"]\n            \n            # Forward pass\n            with torch.amp.autocast(\"cuda\", enabled=(DEVICE==\"cuda\")):\n                ctc_logits, attn_logits, enc_lens = model(inputs, in_lens)\n\n            # Get predictions (CTC probabilities)\n            probs = torch.nn.functional.log_softmax(ctc_logits, dim=-1)\n            probs_np = probs.cpu().detach().numpy()[0]\n\n            # Decode\n            pred_text = decoder.decode(probs_np)\n            label_text = ''.join([list(vocab.keys())[i] for i in labels[0].tolist() if i in vocab.values()])\n\n            all_preds.append(pred_text)\n            all_refs.append(label_text)\n\n    # Compute metrics\n    w = wer(all_refs, all_preds)\n    c = cer(all_refs, all_preds)\n    print(f\"\\nüìä {name} ‚Äî WER: {w:.4f}, CER: {c:.4f}\")\n    return all_refs, all_preds\n\n# Evaluate on validation & test\nval_refs, val_preds = evaluate_dataset(val_dataset, \"Validation\")\ntest_refs, test_preds = evaluate_dataset(test_dataset, \"Test\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:21:36.188658Z","iopub.execute_input":"2025-10-28T11:21:36.189256Z","iopub.status.idle":"2025-10-28T11:25:57.051214Z","shell.execute_reply.started":"2025-10-28T11:21:36.189231Z","shell.execute_reply":"2025-10-28T11:25:57.050356Z"}},"outputs":[{"name":"stderr","text":"Evaluating Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 363/363 [02:04<00:00,  2.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nüìä Validation ‚Äî WER: 1.1351, CER: 4.4331\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Test: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 363/363 [02:16<00:00,  2.67it/s]","output_type":"stream"},{"name":"stdout","text":"\nüìä Test ‚Äî WER: 1.1147, CER: 4.4365\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom jiwer import wer, cer\nfrom tqdm.notebook import tqdm\n\n# Ensure model is in eval mode\nmodel.eval()\n\n# Reverse vocab (id ‚Üí char)\nid2char = {v: k for k, v in vocab.items()}\n\n# --------------- 1Ô∏è‚É£ Greedy CTC decoding ----------------\ndef greedy_ctc_decode(logits, blank_id):\n    \"\"\"\n    logits: (T, vocab_size)\n    blank_id: index of [BLANK] in vocab\n    Returns: list of token IDs (collapsed, blanks removed)\n    \"\"\"\n    pred_ids = np.argmax(logits, axis=-1)\n    prev = blank_id\n    output = []\n    for p in pred_ids:\n        if p != prev and p != blank_id:\n            output.append(p)\n        prev = p\n    return output\n\n# --------------- 2Ô∏è‚É£ Evaluation loop ----------------\npred_texts, ref_texts = [], []\n\nprint(\"‚ö° Running fast greedy decoding on test set...\")\n\nfor batch in tqdm(test_loader, total=len(test_loader)):\n    inputs = batch[\"inputs\"].to(DEVICE)\n    input_lengths = batch[\"input_lengths\"].to(DEVICE)\n    labels = batch[\"labels\"].to(DEVICE)\n    label_lengths = batch[\"label_lengths\"].to(DEVICE)\n\n    with torch.no_grad(), torch.amp.autocast(\"cuda\", enabled=(DEVICE == \"cuda\")):\n        ctc_logits, attn_logits, enc_lens = model(inputs, input_lengths)\n        probs = torch.nn.functional.log_softmax(ctc_logits, dim=-1)\n\n    # Convert to numpy\n    probs_np = probs[0].cpu().numpy()\n\n    # Handle blank token safely\n    if \"[BLANK]\" in vocab:\n        blank_id = vocab[\"[BLANK]\"]\n    elif \"[CTC_BLANK]\" in vocab:\n        blank_id = vocab[\"[CTC_BLANK]\"]\n    else:\n        blank_id = 0  # fallback for safety\n\n    # Decode predictions\n    token_ids = greedy_ctc_decode(probs_np, blank_id)\n    pred_text = \"\".join([id2char[i] for i in token_ids if i in id2char])\n\n    # Decode reference (remove pad/blank/unk)\n    label_np = labels[0].cpu().numpy().tolist()\n    ref_text = \"\".join([\n        id2char[i] for i in label_np\n        if i not in [-100, vocab.get(\"[PAD]\", -1), blank_id, vocab.get(\"[UNK]\", -1)]\n    ])\n\n    pred_texts.append(pred_text)\n    ref_texts.append(ref_text)\n\n# --------------- 3Ô∏è‚É£ Compute metrics ----------------\nfinal_wer = wer(ref_texts, pred_texts)\nfinal_cer = cer(ref_texts, pred_texts)\n\nprint(\"\\nüìä Final Evaluation (Greedy Decode):\")\nprint(f\"   ‚úÖ WER: {final_wer:.4f}\")\nprint(f\"   ‚úÖ CER: {final_cer:.4f}\")\n\n# --------------- 4Ô∏è‚É£ Show sample predictions ----------------\nfor i in range(5):\n    print(f\"\\nüîπ Sample {i+1}\")\n    print(f\"REF:  {ref_texts[i][:250]}\")\n    print(f\"PRED: {pred_texts[i][:250]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T11:45:37.375132Z","iopub.execute_input":"2025-10-28T11:45:37.375984Z","iopub.status.idle":"2025-10-28T11:45:51.142681Z","shell.execute_reply.started":"2025-10-28T11:45:37.375958Z","shell.execute_reply":"2025-10-28T11:45:51.141835Z"}},"outputs":[{"name":"stdout","text":"‚ö° Running fast greedy decoding on test set...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/91 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9ba7c8dba3c4f918f293c7520924a7b"}},"metadata":{}},{"name":"stdout","text":"\nüìä Final Evaluation (Greedy Decode):\n   ‚úÖ WER: 0.6988\n   ‚úÖ CER: 0.1514\n\nüîπ Sample 1\nREF:  sp025-000259_002\t‡§ï‡§æ‡§∞‡•ç‡§Ø‡•á‡§Ω‡§∏‡•ç‡§Æ‡§ø‡§®‡•ç ‡§∏‡§Ç‡§≤‡§ó‡•ç‡§®‡•á‡§≠‡•ç‡§Ø‡§É ‡§∏‡§∞‡•ç‡§µ‡•á‡§≠‡•ç‡§Ø‡§É ‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡§æ‡§≤‡§Ø‡•á‡§≠‡•ç‡§Ø‡§É ‡§∏‡§∞‡•ç‡§µ‡•á‡§≠‡•ç‡§Ø‡§É ‡§µ‡§ø‡§≠‡§æ‡§ó‡•á‡§≠‡•ç‡§Ø‡§É ‡§ï‡•á‡§®‡•ç‡§¶‡•ç‡§∞‡§∏‡•ç‡§Ø ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®‡§æ‡§®‡§æ‡§Æ‡•ç ‡§ö ‡§∏‡§∞‡•ç‡§µ‡•á‡§≠‡•ç‡§Ø‡§É ‡§ï‡§∞‡•ç‡§Æ‡§ï‡§∞‡•á‡§≠‡•ç‡§Ø‡§É ‡§π‡•É‡§¶‡§Ø‡•á‡§® ‡§≠‡•Ç‡§∞‡§ø‡§∂‡§É ‡§µ‡§∞‡•ç‡§ß‡§æ‡§™‡§®‡§æ‡§®‡§ø ‡§µ‡§ø‡§§‡§∞‡§æ‡§Æ‡§ø\nPRED: sp025-0007_002\t‡§§‡§æ‡§∞‡•ç‡§Ø‡•á‡§∏‡•ç‡§Æ‡§ø‡§®‡•ç ‡§∏‡§≤‡§≤‡•ç‡§≤‡§¶‡•ç‡§®‡•á‡§≠‡•ç‡§Ø‡§æ‡§É ‡§∏‡§∞‡•ç‡§µ‡•á‡§≠‡•ç‡§Ø‡§É ‡§Æ‡§®‡•ç‡§§‡•ç‡§∞‡§æ‡§≤‡§Ø‡•à‡§≠‡•ç‡§Ø‡§É ‡§∏‡§∞‡•ç‡§µ‡•á‡§≠‡•ç‡§Ø ‡§µ‡§ø‡§≠‡§æ‡§¶‡•á‡§≠‡•ç‡§Ø‡§É ‡§§‡•á‡§®‡•ç‡•ç‡§∞‡§∏‡•ç‡§Ø ‡§®‡§æ‡§ú‡•ç‡§Ø ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§®‡§æ‡§®‡§æ‡§Æ‡•ç ‡§ö ‡§∏‡§∞‡•ç‡§µ‡•á‡§≠‡•ç ‡§ï‡§∞‡•ç‡§Æ ‡§ï‡§∞‡•á‡§≠‡•ç‡§Ø‡§É ‡§µ‡§ø‡§¶‡•á‡§® ‡§≠‡•Ç‡§∞‡§ø‡§∑‡§É ‡§µ‡§∞‡•ç‡§ß‡§æ‡§™‡§®‡§æ‡§®‡§ø ‡§µ‡§ø‡§§‡§∞‡§æ‡§Æ‡§ø\n\nüîπ Sample 2\nREF:  sp002-001298_RV_02\t‡§Ö‡§∞‡•Å‡§®‡•ç‡§ß‡§§‡•Ä‡§Æ‡•ç ‡§ö ‡§∏‡§µ‡§§‡•ç‡§∏‡§æ‡§Æ‡•ç ‡§ß‡•á‡§®‡•Å‡§Æ‡•ç ‡§ö ‡§™‡•ç‡§∞‡§¶‡§ï‡•ç‡§∑‡§ø‡§£‡•Ä‡§ï‡•É‡§§‡•ç‡§Ø ‡§™‡•ç‡§∞‡§ó‡§§‡•ã ‡§¶‡§ï‡•ç‡§∑‡§ø‡§£‡§Æ‡•ç ‡§™‡•ç‡§∞‡§¶‡§ï‡•ç‡§∑‡§ø‡§£‡§Æ‡•ç\nPRED: sp002-0017_RV_02\t‡§Ö‡§∞‡•Å‡§®‡•ç‡§ß‡§§‡•Ä‡§û‡•ç ‡§ö ‡§∏‡§µ‡§§‡•ç‡§∏‡§∏‡§æ‡§Æ‡•ç ‡§¶‡•á‡§®‡•Å‡§û‡•ç ‡§ö ‡§™‡•ç‡§∞‡§¶‡§ï‡•ç‡§∑‡§ø‡§£‡•á ‡§ï‡•É‡§§‡•ç‡§Ø ‡§™‡•ç‡§∞‡§ó‡§§‡•ã ‡§¶‡§ï‡•ç‡§∑‡§ø‡§£‡§Æ‡•ç ‡§™‡•ç‡§∞‡§¶‡§ï‡•ç‡§∑‡§ø‡§£‡§Æ‡•ç\n\nüîπ Sample 3\nREF:  sp025-000381_002\t‡§Ö‡§∏‡•ç‡§Æ‡§¶‡•Ä‡§Ø‡§æ‡§É ‡§Ø‡•á ‡§®‡§ø‡§∞‡•ç‡§ß‡§®‡§æ‡§É ‡§∂‡§ø‡§≤‡•ç‡§™‡§ø‡§®‡§É ‡§Æ‡•Ç‡§∞‡•ç‡§§‡§ø ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§§‡§æ‡§∞‡§É ‡§∏‡§®‡•ç‡§§‡§ø ‡§§‡•á ‡§Ü‡§ú‡•Ä‡§µ‡§ø‡§ï‡§æ‡§Æ‡•ç ‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§∏‡•ç‡§Ø‡§®‡•ç‡§§‡§ø\nPRED: sp025-00032_00\t‡§∏‡§Æ‡§¶‡•Ä‡§Ø‡§æ‡§æ‡§Ø‡•á‡§®‡§ø‡§∞‡•ç‡§ß‡§®‡§æ‡§∂‡•Ä‡§™‡§ø‡§¶‡§É ‡§Æ‡•Ç‡§§‡§ø ‡§®‡§ø‡§∞‡•ç‡§Æ‡§æ‡§§‡§æ‡§∞‡§É ‡§∏‡§®‡•ç‡§§‡§ø‡§§‡•á ‡§Ü‡§ú‡•Ä‡§∞‡§ø‡§ï‡§æ‡§Æ‡•ç ‡§™‡•ç‡§∞‡§æ‡§∏‡•ç‡§Ø‡§®‡•ç‡§§‡§ø\n\nüîπ Sample 4\nREF:  sp002-001148_RV_02\t‡§® ‡§§‡•Å ‡§§‡§µ ‡§á‡§§‡•ç‡§Ø‡§∞‡•ç‡§•‡§É\nPRED: sp002-0017_RV_02\t‡§Æ‡§§‡•Å ‡§§‡§µ ‡§á‡§§‡•ç‡§Ø‡§∞‡•ç‡§•‡§É\n\nüîπ Sample 5\nREF:  sp002-002455_RV_07\t‡§∏‡•Ç‡§∞‡•ç‡§Ø‡§Æ‡•ç ‡§â‡§™‡§∞‡•Å‡§∞‡•ã‡§ß ‡§Ü‡§ö‡•ç‡§õ‡§æ‡§¶‡§Ø‡§æ‡§Æ‡§æ‡§∏\nPRED: sp002-000_RV_05\t‡§∏‡•Ç‡§∞‡•ç‡§Ø‡§Æ‡•ç ‡§â‡§™‡§∞‡•Å‡§∞‡•ã‡§ß‡§æ ‡§Ü‡•ç‡§æ‡§¶‡§Ø‡§æ‡§Æ‡§æ‡§∏\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}